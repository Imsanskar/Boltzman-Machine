{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6ba0d040",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torchvision.utils import make_grid , save_image\n",
    "from boltzman_pytorch import *\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "40bc2c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = RBM(k = 1)\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.05)\n",
    "loss_fn.to(device)\n",
    "model.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "627e35f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((28, 28)),\n",
    "    transforms.Grayscale()\n",
    "])\n",
    "handwritten_data = torchvision.datasets.ImageFolder(\"./data/handwrittendataset/Train/\", transform)\n",
    "dataloader = torch.utils.data.DataLoader(dataset=handwritten_data, batch_size = 2, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "6be836a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss for 0 epoch: 201.29074096679688\n",
      "Training loss for 0 epoch: 232.6512451171875\n",
      "Training loss for 0 epoch: 235.06634521484375\n",
      "Training loss for 0 epoch: 234.8860321044922\n",
      "Training loss for 0 epoch: 235.12005615234375\n",
      "Training loss for 0 epoch: 234.9910430908203\n",
      "Training loss for 0 epoch: 234.86968994140625\n",
      "Training loss for 0 epoch: 234.9958953857422\n",
      "Training loss for 0 epoch: 235.1533203125\n",
      "Training loss for 0 epoch: 235.0968017578125\n",
      "Training loss for 0 epoch: 235.27362060546875\n",
      "Training loss for 0 epoch: 235.20420837402344\n",
      "Training loss for 0 epoch: 235.28614807128906\n",
      "Training loss for 0 epoch: 235.39901733398438\n",
      "Training loss for 0 epoch: 235.63522338867188\n",
      "Training loss for 0 epoch: 235.7266845703125\n",
      "Training loss for 0 epoch: 235.69358825683594\n",
      "Training loss for 0 epoch: 235.28504943847656\n",
      "Training loss for 0 epoch: 235.19613647460938\n",
      "Training loss for 0 epoch: 235.16293334960938\n",
      "Training loss for 0 epoch: 235.1869354248047\n",
      "Training loss for 0 epoch: 235.1509246826172\n",
      "Training loss for 0 epoch: 235.15109252929688\n",
      "Training loss for 0 epoch: 235.30638122558594\n",
      "Training loss for 0 epoch: 235.2574462890625\n",
      "Training loss for 0 epoch: 235.2226104736328\n",
      "Training loss for 0 epoch: 235.1109619140625\n",
      "Training loss for 0 epoch: 235.1458740234375\n",
      "Training loss for 0 epoch: 235.0931396484375\n",
      "Training loss for 0 epoch: 235.0837860107422\n",
      "Training loss for 0 epoch: 235.08726501464844\n",
      "Training loss for 0 epoch: 235.00711059570312\n",
      "Training loss for 0 epoch: 235.1064453125\n",
      "Training loss for 0 epoch: 235.07383728027344\n",
      "Training loss for 0 epoch: 235.02182006835938\n",
      "Training loss for 0 epoch: 234.9286651611328\n",
      "Training loss for 0 epoch: 234.94366455078125\n",
      "Training loss for 0 epoch: 235.01675415039062\n",
      "Training loss for 0 epoch: 235.0086212158203\n",
      "Training loss for 0 epoch: 235.07447814941406\n",
      "Training loss for 0 epoch: 235.17620849609375\n",
      "Training loss for 0 epoch: 235.14849853515625\n",
      "Training loss for 0 epoch: 235.055908203125\n",
      "Training loss for 0 epoch: 235.0527801513672\n",
      "Training loss for 0 epoch: 235.0327606201172\n",
      "Training loss for 0 epoch: 235.2303924560547\n",
      "Training loss for 0 epoch: 235.21902465820312\n",
      "Training loss for 0 epoch: 235.29615783691406\n",
      "Training loss for 0 epoch: 235.3432159423828\n",
      "Training loss for 0 epoch: 235.36318969726562\n",
      "Training loss for 0 epoch: 235.42381286621094\n",
      "Training loss for 0 epoch: 235.48216247558594\n",
      "Training loss for 0 epoch: 235.52671813964844\n",
      "Training loss for 0 epoch: 235.5884552001953\n",
      "Training loss for 0 epoch: 235.56007385253906\n",
      "Training loss for 0 epoch: 235.56906127929688\n",
      "Training loss for 0 epoch: 235.5584259033203\n",
      "Training loss for 0 epoch: 235.5929718017578\n",
      "Training loss for 0 epoch: 235.56492614746094\n",
      "Training loss for 0 epoch: 235.50601196289062\n",
      "Training loss for 0 epoch: 235.5774383544922\n",
      "Training loss for 0 epoch: 235.57113647460938\n",
      "Training loss for 0 epoch: 235.55514526367188\n",
      "Training loss for 0 epoch: 235.5907440185547\n",
      "Training loss for 0 epoch: 235.55516052246094\n",
      "Training loss for 0 epoch: 235.60910034179688\n",
      "Training loss for 0 epoch: 235.66885375976562\n",
      "Training loss for 0 epoch: 235.5923309326172\n",
      "Training loss for 0 epoch: 235.57630920410156\n",
      "Training loss for 0 epoch: 235.63755798339844\n",
      "Training loss for 0 epoch: 235.65887451171875\n",
      "Training loss for 0 epoch: 235.701171875\n",
      "Training loss for 0 epoch: 235.70684814453125\n",
      "Training loss for 0 epoch: 235.7368927001953\n",
      "Training loss for 0 epoch: 235.69212341308594\n",
      "Training loss for 0 epoch: 235.65895080566406\n",
      "Training loss for 0 epoch: 235.66600036621094\n",
      "Training loss for 0 epoch: 235.69517517089844\n",
      "Training loss for 0 epoch: 235.63970947265625\n",
      "Training loss for 0 epoch: 235.66799926757812\n",
      "Training loss for 0 epoch: 235.72731018066406\n",
      "Training loss for 0 epoch: 235.74290466308594\n",
      "Training loss for 0 epoch: 235.76068115234375\n",
      "Training loss for 0 epoch: 235.74893188476562\n",
      "Training loss for 0 epoch: 235.79454040527344\n",
      "Training loss for 0 epoch: 235.86151123046875\n",
      "Training loss for 0 epoch: 235.89892578125\n",
      "Training loss for 0 epoch: 235.8895721435547\n",
      "Training loss for 0 epoch: 235.86402893066406\n",
      "Training loss for 0 epoch: 235.78855895996094\n",
      "Training loss for 0 epoch: 235.81671142578125\n",
      "Training loss for 0 epoch: 235.79644775390625\n",
      "Training loss for 0 epoch: 235.81243896484375\n",
      "Training loss for 0 epoch: 235.80752563476562\n",
      "Training loss for 0 epoch: 235.812255859375\n",
      "Training loss for 0 epoch: 235.7756805419922\n",
      "Training loss for 0 epoch: 235.75973510742188\n",
      "Training loss for 0 epoch: 235.76133728027344\n",
      "Training loss for 0 epoch: 235.76937866210938\n",
      "Training loss for 0 epoch: 235.81822204589844\n",
      "Training loss for 0 epoch: 235.78402709960938\n",
      "Training loss for 0 epoch: 235.75167846679688\n",
      "Training loss for 0 epoch: 235.73904418945312\n",
      "Training loss for 0 epoch: 235.7463836669922\n",
      "Training loss for 0 epoch: 235.7316131591797\n",
      "Training loss for 0 epoch: 235.733642578125\n",
      "Training loss for 0 epoch: 235.7389678955078\n",
      "Training loss for 0 epoch: 235.73043823242188\n",
      "Training loss for 0 epoch: 235.69248962402344\n",
      "Training loss for 0 epoch: 235.66778564453125\n",
      "Training loss for 0 epoch: 235.7049560546875\n",
      "Training loss for 0 epoch: 235.7188262939453\n",
      "Training loss for 0 epoch: 235.6992645263672\n",
      "Training loss for 0 epoch: 235.66683959960938\n",
      "Training loss for 0 epoch: 235.6281280517578\n",
      "Training loss for 0 epoch: 235.6342010498047\n",
      "Training loss for 0 epoch: 235.62429809570312\n",
      "Training loss for 0 epoch: 235.6411895751953\n",
      "Training loss for 0 epoch: 235.6756134033203\n",
      "Training loss for 0 epoch: 235.67247009277344\n",
      "Training loss for 0 epoch: 235.68389892578125\n",
      "Training loss for 0 epoch: 235.6925048828125\n",
      "Training loss for 0 epoch: 235.64744567871094\n",
      "Training loss for 0 epoch: 235.67527770996094\n",
      "Training loss for 0 epoch: 235.67784118652344\n",
      "Training loss for 0 epoch: 235.6211700439453\n",
      "Training loss for 0 epoch: 235.6229705810547\n",
      "Training loss for 0 epoch: 235.6115264892578\n",
      "Training loss for 0 epoch: 235.61358642578125\n",
      "Training loss for 0 epoch: 235.62185668945312\n",
      "Training loss for 0 epoch: 235.6260986328125\n",
      "Training loss for 0 epoch: 235.6102752685547\n",
      "Training loss for 0 epoch: 235.6248016357422\n",
      "Training loss for 0 epoch: 235.61468505859375\n",
      "Training loss for 0 epoch: 235.62081909179688\n",
      "Training loss for 0 epoch: 235.62149047851562\n",
      "Training loss for 0 epoch: 235.5762481689453\n",
      "Training loss for 0 epoch: 235.55479431152344\n",
      "Training loss for 0 epoch: 235.54388427734375\n",
      "Training loss for 0 epoch: 235.5353546142578\n",
      "Training loss for 0 epoch: 235.51681518554688\n",
      "Training loss for 0 epoch: 235.51666259765625\n",
      "Training loss for 0 epoch: 235.5020294189453\n",
      "Training loss for 0 epoch: 235.5157928466797\n",
      "Training loss for 0 epoch: 235.49229431152344\n",
      "Training loss for 0 epoch: 235.44659423828125\n",
      "Training loss for 0 epoch: 235.4680938720703\n",
      "Training loss for 0 epoch: 235.4673614501953\n",
      "Training loss for 0 epoch: 235.4502716064453\n",
      "Training loss for 0 epoch: 235.46238708496094\n",
      "Training loss for 0 epoch: 235.4852294921875\n",
      "Training loss for 0 epoch: 235.44773864746094\n",
      "Training loss for 0 epoch: 235.42575073242188\n",
      "Training loss for 0 epoch: 235.42031860351562\n",
      "Training loss for 0 epoch: 235.43910217285156\n",
      "Training loss for 0 epoch: 235.41929626464844\n",
      "Training loss for 0 epoch: 235.42889404296875\n",
      "Training loss for 0 epoch: 235.40029907226562\n",
      "Training loss for 0 epoch: 235.4097137451172\n",
      "Training loss for 0 epoch: 235.40988159179688\n",
      "Training loss for 0 epoch: 235.40963745117188\n",
      "Training loss for 0 epoch: 235.40501403808594\n",
      "Training loss for 0 epoch: 235.40672302246094\n",
      "Training loss for 0 epoch: 235.4340057373047\n",
      "Training loss for 0 epoch: 235.4554901123047\n",
      "Training loss for 0 epoch: 235.4554443359375\n",
      "Training loss for 0 epoch: 235.45497131347656\n",
      "Training loss for 0 epoch: 235.40945434570312\n",
      "Training loss for 0 epoch: 235.3803253173828\n",
      "Training loss for 0 epoch: 235.3673095703125\n",
      "Training loss for 0 epoch: 235.36788940429688\n",
      "Training loss for 0 epoch: 235.35194396972656\n",
      "Training loss for 0 epoch: 235.33580017089844\n",
      "Training loss for 0 epoch: 235.3423309326172\n",
      "Training loss for 0 epoch: 235.34002685546875\n",
      "Training loss for 0 epoch: 235.33900451660156\n",
      "Training loss for 0 epoch: 235.3459014892578\n",
      "Training loss for 0 epoch: 235.35223388671875\n",
      "Training loss for 0 epoch: 235.3649444580078\n",
      "Training loss for 0 epoch: 235.35757446289062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss for 0 epoch: 235.36553955078125\n",
      "Training loss for 0 epoch: 235.34584045410156\n",
      "Training loss for 0 epoch: 235.3564910888672\n",
      "Training loss for 0 epoch: 235.34762573242188\n",
      "Training loss for 0 epoch: 235.3636016845703\n",
      "Training loss for 0 epoch: 235.3501434326172\n",
      "Training loss for 0 epoch: 235.36204528808594\n",
      "Training loss for 0 epoch: 235.3807830810547\n",
      "Training loss for 0 epoch: 235.36952209472656\n",
      "Training loss for 0 epoch: 235.3596649169922\n",
      "Training loss for 0 epoch: 235.3572998046875\n",
      "Training loss for 0 epoch: 235.35501098632812\n",
      "Training loss for 0 epoch: 235.35623168945312\n",
      "Training loss for 0 epoch: 235.37091064453125\n",
      "Training loss for 0 epoch: 235.36178588867188\n",
      "Training loss for 0 epoch: 235.3441925048828\n",
      "Training loss for 0 epoch: 235.33082580566406\n",
      "Training loss for 0 epoch: 235.3260498046875\n",
      "Training loss for 0 epoch: 235.33526611328125\n",
      "Training loss for 0 epoch: 235.33583068847656\n",
      "Training loss for 0 epoch: 235.33786010742188\n",
      "Training loss for 0 epoch: 235.34877014160156\n",
      "Training loss for 0 epoch: 235.33094787597656\n",
      "Training loss for 0 epoch: 235.3259735107422\n",
      "Training loss for 0 epoch: 235.33885192871094\n",
      "Training loss for 0 epoch: 235.337158203125\n",
      "Training loss for 0 epoch: 235.3468475341797\n",
      "Training loss for 0 epoch: 235.33602905273438\n",
      "Training loss for 0 epoch: 235.3289794921875\n",
      "Training loss for 0 epoch: 235.30799865722656\n",
      "Training loss for 0 epoch: 235.3282470703125\n",
      "Training loss for 0 epoch: 235.3091583251953\n",
      "Training loss for 0 epoch: 235.29730224609375\n",
      "Training loss for 0 epoch: 235.31463623046875\n",
      "Training loss for 0 epoch: 235.3214569091797\n",
      "Training loss for 0 epoch: 235.32858276367188\n",
      "Training loss for 0 epoch: 235.32354736328125\n",
      "Training loss for 0 epoch: 235.325927734375\n",
      "Training loss for 0 epoch: 235.33546447753906\n",
      "Training loss for 0 epoch: 235.3155059814453\n",
      "Training loss for 0 epoch: 235.3028106689453\n",
      "Training loss for 0 epoch: 235.30862426757812\n",
      "Training loss for 0 epoch: 235.32015991210938\n",
      "Training loss for 0 epoch: 235.33181762695312\n",
      "Training loss for 0 epoch: 235.33229064941406\n",
      "Training loss for 0 epoch: 235.32513427734375\n",
      "Training loss for 0 epoch: 235.33169555664062\n",
      "Training loss for 0 epoch: 235.3339080810547\n",
      "Training loss for 0 epoch: 235.30885314941406\n",
      "Training loss for 0 epoch: 235.2757568359375\n",
      "Training loss for 0 epoch: 235.27560424804688\n",
      "Training loss for 0 epoch: 235.29269409179688\n",
      "Training loss for 0 epoch: 235.29977416992188\n",
      "Training loss for 0 epoch: 235.2805938720703\n",
      "Training loss for 0 epoch: 235.2648162841797\n",
      "Training loss for 0 epoch: 235.23814392089844\n",
      "Training loss for 0 epoch: 235.23318481445312\n",
      "Training loss for 0 epoch: 235.2322540283203\n",
      "Training loss for 0 epoch: 235.22666931152344\n",
      "Training loss for 0 epoch: 235.22865295410156\n",
      "Training loss for 0 epoch: 235.23252868652344\n",
      "Training loss for 0 epoch: 235.21449279785156\n",
      "Training loss for 0 epoch: 235.21348571777344\n",
      "Training loss for 0 epoch: 235.21859741210938\n",
      "Training loss for 0 epoch: 235.21917724609375\n",
      "Training loss for 0 epoch: 235.2207489013672\n",
      "Training loss for 0 epoch: 235.2282257080078\n",
      "Training loss for 0 epoch: 235.21865844726562\n",
      "Training loss for 0 epoch: 235.2287139892578\n",
      "Training loss for 0 epoch: 235.21461486816406\n",
      "Training loss for 0 epoch: 235.21783447265625\n",
      "Training loss for 0 epoch: 235.21163940429688\n",
      "Training loss for 0 epoch: 235.2145538330078\n",
      "Training loss for 0 epoch: 235.21446228027344\n",
      "Training loss for 0 epoch: 235.2262725830078\n",
      "Training loss for 0 epoch: 235.2248992919922\n",
      "Training loss for 0 epoch: 235.2213592529297\n",
      "Training loss for 0 epoch: 235.21754455566406\n",
      "Training loss for 0 epoch: 235.22677612304688\n",
      "Training loss for 0 epoch: 235.22921752929688\n",
      "Training loss for 0 epoch: 235.24319458007812\n",
      "Training loss for 0 epoch: 235.2642364501953\n",
      "Training loss for 0 epoch: 235.26451110839844\n",
      "Training loss for 0 epoch: 235.28106689453125\n",
      "Training loss for 0 epoch: 235.28146362304688\n",
      "Training loss for 0 epoch: 235.26527404785156\n",
      "Training loss for 0 epoch: 235.26092529296875\n",
      "Training loss for 0 epoch: 235.27322387695312\n",
      "Training loss for 0 epoch: 235.26922607421875\n",
      "Training loss for 0 epoch: 235.26036071777344\n",
      "Training loss for 0 epoch: 235.26634216308594\n",
      "Training loss for 0 epoch: 235.2471923828125\n",
      "Training loss for 0 epoch: 235.2484130859375\n",
      "Training loss for 0 epoch: 235.24928283691406\n",
      "Training loss for 0 epoch: 235.2352294921875\n",
      "Training loss for 0 epoch: 235.24435424804688\n",
      "Training loss for 0 epoch: 235.24151611328125\n",
      "Training loss for 0 epoch: 235.22906494140625\n",
      "Training loss for 0 epoch: 235.2276153564453\n",
      "Training loss for 0 epoch: 235.2252197265625\n",
      "Training loss for 0 epoch: 235.2185821533203\n",
      "Training loss for 0 epoch: 235.2212371826172\n",
      "Training loss for 0 epoch: 235.2189483642578\n",
      "Training loss for 0 epoch: 235.20155334472656\n",
      "Training loss for 0 epoch: 235.21734619140625\n",
      "Training loss for 0 epoch: 235.22149658203125\n",
      "Training loss for 0 epoch: 235.21890258789062\n",
      "Training loss for 0 epoch: 235.22305297851562\n",
      "Training loss for 0 epoch: 235.23760986328125\n",
      "Training loss for 0 epoch: 235.2327880859375\n",
      "Training loss for 0 epoch: 235.23741149902344\n",
      "Training loss for 0 epoch: 235.24220275878906\n",
      "Training loss for 0 epoch: 235.23973083496094\n",
      "Training loss for 0 epoch: 235.2444610595703\n",
      "Training loss for 0 epoch: 235.2489471435547\n",
      "Training loss for 0 epoch: 235.23751831054688\n",
      "Training loss for 0 epoch: 235.23516845703125\n",
      "Training loss for 0 epoch: 235.22518920898438\n",
      "Training loss for 0 epoch: 235.24232482910156\n",
      "Training loss for 0 epoch: 235.24420166015625\n",
      "Training loss for 0 epoch: 235.25912475585938\n",
      "Training loss for 0 epoch: 235.2606964111328\n",
      "Training loss for 0 epoch: 235.25221252441406\n",
      "Training loss for 0 epoch: 235.2524871826172\n",
      "Training loss for 0 epoch: 235.27203369140625\n",
      "Training loss for 0 epoch: 235.2521209716797\n",
      "Training loss for 0 epoch: 235.26548767089844\n",
      "Training loss for 0 epoch: 235.26231384277344\n",
      "Training loss for 0 epoch: 235.26878356933594\n",
      "Training loss for 0 epoch: 235.27125549316406\n",
      "Training loss for 0 epoch: 235.2871856689453\n",
      "Training loss for 0 epoch: 235.28379821777344\n",
      "Training loss for 0 epoch: 235.28009033203125\n",
      "Training loss for 0 epoch: 235.27609252929688\n",
      "Training loss for 0 epoch: 235.2770538330078\n",
      "Training loss for 0 epoch: 235.26687622070312\n",
      "Training loss for 0 epoch: 235.26651000976562\n",
      "Training loss for 0 epoch: 235.24745178222656\n",
      "Training loss for 0 epoch: 235.24244689941406\n",
      "Training loss for 0 epoch: 235.2503662109375\n",
      "Training loss for 0 epoch: 235.25869750976562\n",
      "Training loss for 0 epoch: 235.26620483398438\n",
      "Training loss for 0 epoch: 235.26344299316406\n",
      "Training loss for 0 epoch: 235.2576904296875\n",
      "Training loss for 0 epoch: 235.23580932617188\n",
      "Training loss for 0 epoch: 235.23121643066406\n",
      "Training loss for 0 epoch: 235.220703125\n",
      "Training loss for 0 epoch: 235.21751403808594\n",
      "Training loss for 0 epoch: 235.19943237304688\n",
      "Training loss for 0 epoch: 235.1929473876953\n",
      "Training loss for 0 epoch: 235.18255615234375\n",
      "Training loss for 0 epoch: 235.18106079101562\n",
      "Training loss for 0 epoch: 235.18186950683594\n",
      "Training loss for 0 epoch: 235.1820831298828\n",
      "Training loss for 0 epoch: 235.18663024902344\n",
      "Training loss for 0 epoch: 235.18081665039062\n",
      "Training loss for 0 epoch: 235.18484497070312\n",
      "Training loss for 0 epoch: 235.1914825439453\n",
      "Training loss for 0 epoch: 235.19775390625\n",
      "Training loss for 0 epoch: 235.1966552734375\n",
      "Training loss for 0 epoch: 235.20579528808594\n",
      "Training loss for 0 epoch: 235.20675659179688\n",
      "Training loss for 0 epoch: 235.2001953125\n",
      "Training loss for 0 epoch: 235.19699096679688\n",
      "Training loss for 0 epoch: 235.2044219970703\n",
      "Training loss for 0 epoch: 235.20419311523438\n",
      "Training loss for 0 epoch: 235.20196533203125\n",
      "Training loss for 0 epoch: 235.21420288085938\n",
      "Training loss for 0 epoch: 235.21302795410156\n",
      "Training loss for 0 epoch: 235.2086181640625\n",
      "Training loss for 0 epoch: 235.20379638671875\n",
      "Training loss for 0 epoch: 235.1892852783203\n",
      "Training loss for 0 epoch: 235.18833923339844\n",
      "Training loss for 0 epoch: 235.20045471191406\n",
      "Training loss for 0 epoch: 235.2169647216797\n",
      "Training loss for 0 epoch: 235.22068786621094\n",
      "Training loss for 0 epoch: 235.21449279785156\n",
      "Training loss for 0 epoch: 235.2046661376953\n",
      "Training loss for 0 epoch: 235.2056427001953\n",
      "Training loss for 0 epoch: 235.20419311523438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss for 0 epoch: 235.20108032226562\n",
      "Training loss for 0 epoch: 235.20169067382812\n",
      "Training loss for 0 epoch: 235.20347595214844\n",
      "Training loss for 0 epoch: 235.20016479492188\n",
      "Training loss for 0 epoch: 235.1974639892578\n",
      "Training loss for 0 epoch: 235.19834899902344\n",
      "Training loss for 0 epoch: 235.1986541748047\n",
      "Training loss for 0 epoch: 235.20370483398438\n",
      "Training loss for 0 epoch: 235.20526123046875\n",
      "Training loss for 0 epoch: 235.20761108398438\n",
      "Training loss for 0 epoch: 235.21080017089844\n",
      "Training loss for 0 epoch: 235.20664978027344\n",
      "Training loss for 0 epoch: 235.222412109375\n",
      "Training loss for 0 epoch: 235.21444702148438\n",
      "Training loss for 0 epoch: 235.21585083007812\n",
      "Training loss for 0 epoch: 235.23106384277344\n",
      "Training loss for 0 epoch: 235.2297821044922\n",
      "Training loss for 0 epoch: 235.22813415527344\n",
      "Training loss for 0 epoch: 235.2278289794922\n",
      "Training loss for 0 epoch: 235.22962951660156\n",
      "Training loss for 0 epoch: 235.22854614257812\n",
      "Training loss for 0 epoch: 235.2431640625\n",
      "Training loss for 0 epoch: 235.24847412109375\n",
      "Training loss for 0 epoch: 235.24530029296875\n",
      "Training loss for 0 epoch: 235.2328643798828\n",
      "Training loss for 0 epoch: 235.23236083984375\n",
      "Training loss for 0 epoch: 235.2314453125\n",
      "Training loss for 0 epoch: 235.23658752441406\n",
      "Training loss for 0 epoch: 235.23715209960938\n",
      "Training loss for 0 epoch: 235.2374725341797\n",
      "Training loss for 0 epoch: 235.23226928710938\n",
      "Total Data = 39099\n"
     ]
    }
   ],
   "source": [
    "rbm = RBM(k=1)\n",
    "loss_boltzman = np.array(loss_boltzman)\n",
    "\n",
    "for epoch in range(1):\n",
    "    loss_ = []\n",
    "    for i, (data,target) in enumerate(dataloader):\n",
    "        data = Variable(data.view(data.size()[0],784))\n",
    "        sample_data = data.bernoulli()\n",
    "        \n",
    "        v,v1 = rbm(sample_data)\n",
    "        loss = rbm.free_energy(v) - rbm.free_energy(v1)\n",
    "        loss_.append(loss.data)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(\"Training loss for {} epoch: {}\".format(epoch, np.mean(loss_)))\n",
    "            \n",
    "    print(f\"Total Data = {i}\")\n",
    "            \n",
    "    loss_boltzman = np.append(loss_boltzman, np.mean(loss_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "684f599f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_adn_save(file_name,img):\n",
    "    npimg = np.transpose(img.numpy(),(1,2,0))\n",
    "    f = \"./%s.png\" % file_name\n",
    "    plt.imshow(npimg)\n",
    "    plt.imsave(f,npimg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "dc98bcaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_adn_save(\"real\",make_grid(v.view(v.size()[0],1,28,28).data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "bab0c63d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.9320e-01,\n",
       "            3.8892e-02, 0.0000e+00],\n",
       "           [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 9.3670e-01,\n",
       "            1.9320e-01, 0.0000e+00],\n",
       "           ...,\n",
       "           [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00]]],\n",
       " \n",
       " \n",
       "         [[[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.8305e-02,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [0.0000e+00, 3.6011e-03, 6.1819e-02,  ..., 5.7917e-02,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           ...,\n",
       "           [0.0000e+00, 0.0000e+00, 2.1406e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [0.0000e+00, 0.0000e+00, 3.0009e-04,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00]]]]),\n",
       " tensor([ 0, 26])]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7134fefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(rbm.state_dict(), \"saved_models/rbm_nepali_characters.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minor",
   "language": "python",
   "name": "minor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
